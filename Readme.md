# Ambiente Delta/Spark

Este projeto cria um ambiente local para desenvolvimento e testes com **Apache Spark**, **Delta Lake**, **MinIO (S3 local)** e **Jupyter Notebook**, usando **Docker Compose**.

Ideal para estudar e testar leitura e escrita de arquivos Delta com Spark, em um bucket S3 local simulado via MinIO.

---

‚úÖ Tecnologias Utilizadas
Jupyter Notebook (pyspark-notebook:spark-3.5.0)
Imagem base com PySpark pr√©-instalado, usada como ambiente interativo para testes com Spark.

Apache Spark 3.5.0
Motor de processamento distribu√≠do usado para an√°lise e transforma√ß√£o de dados.

Delta Lake 3.2.0 (Scala 2.12)
Formato de armazenamento transacional otimizado para grandes volumes de dados com suporte a ACID.

MinIO (latest)
Servi√ßo de armazenamento compat√≠vel com S3 para uso local, simulando um bucket da AWS.

Hadoop AWS 3.3.4
Conector usado pelo Spark para acessar sistemas de arquivos compat√≠veis com S3 via fs.s3a.

AWS Java SDK 1.12.262
Depend√™ncia necess√°ria para comunica√ß√£o entre o Hadoop e servi√ßos compat√≠veis com S3.

Docker (vers√£o 20 ou superior)
Plataforma de cont√™ineres usada para empacotar e executar os servi√ßos.

Docker Compose (vers√£o 1.29 ou superior)
Ferramenta para orquestrar m√∫ltiplos cont√™ineres (Jupyter, Spark e MinIO) com um √∫nico comando.


## üöÄ Como subir o ambiente

docker-compose up --build

Isso criar√° os servi√ßos: notebook, minio, e spark.

üß≠ Como acessar o projeto

Acessar o Jupyter Notebook
Navegue at√©: http://localhost:8888

Abra um novo terminal (sem parar o ambiente) e execute:

docker-compose logs notebook

Procure por uma linha parecida com esta no log:
http://127.0.0.1:8888/?token=abc123def456...
Essa URL cont√©m o token de acesso. Copie o conteudo a partir de token=

ü™£ Como criar um bucket no MinIO
Acesse http://localhost:9001

Clique em "Create Bucket"

Nomeie como: meu-bucket (ou outro nome que preferir)

Use este nome nos caminhos do Spark com s3a://meu-bucket/

‚úçÔ∏è Exemplo: lendo e escrevendo arquivos Delta com Spark
Abra um notebook Jupyter e execute:

from pyspark.sql import SparkSession

# Sess√£o Spark com configura√ß√£o S3A (MinIO)
spark = (
    SparkSession.builder.appName("DeltaS3Example")
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin")
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin")
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    .getOrCreate()
)

# Criar DataFrame
df = spark.createDataFrame([
    ("Jo√£o", 28),
    ("Maria", 34),
    ("Carlos", 41)
], ["nome", "idade"])

# Salvar no MinIO em formato Delta
df.write.format("delta").mode("overwrite").save("s3a://meu-bucket/delta/clientes")

# Ler o mesmo arquivo Delta
df_lido = spark.read.format("delta").load("s3a://meu-bucket/delta/clientes")
df_lido.show()

üîê Seguran√ßa
Este ambiente √© projetado para uso local apenas. Para facilitar as senhas est√£o em hardcode (N√£o replicar em ambientes de produ√ß√£o)



